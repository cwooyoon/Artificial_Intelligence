# Back Propagation

출처: 모두의 딥러닝(조태호 저)

## 오차 역전파의 개념

단일 퍼셉트론에서 결괏값을 얻으면 오차를 구해 이를 토대로 앞 단계에서 정한 가중치를 조정함

그림 8-1  단일 퍼셉트론에서의 오차 수정

<img src="https://user-images.githubusercontent.com/54765256/90968830-1745e600-e52c-11ea-809f-a6c6ad31a3d5.png">

다층 퍼셉트론 역시 결괏값의 오차를 구해 이를 토대로 하나 앞선 가중치를 차례로 거슬러 올라가며 조정함

그림 8-2  다층 퍼셉트론에서의 오차 수정

<img src="https://user-images.githubusercontent.com/54765256/90968838-33498780-e52c-11ea-9367-26fe9a5ffc32.png">

오차 역전파(back propagation) :
     다층 퍼셉트론에서의 최적화 과정
     
오차 역전파 구동 방식은 다음과 같이 정리할 수 있음

     1 | 임의의 초기 가중치( W(1) )를 준 뒤 결과( y(out) )를 계산함
     
     2 | 계산 결과와 우리가 원하는 값 사이의 오차를 구함
     
     3 | 경사 하강법을 이용해 바로 앞 가중치를 오차가 작아지는 방향으로 업데이트함
     
     4 | 위 과정을 더이상 오차가 줄어들지 않을 때까지 반복함

여기서 ‘오차가 작아지는 방향으로 업데이트한다’는 의미는 미분 값이 0에 가까워지는 방향으로 나아간다는 말임, 
즉, ‘기울기가 0이 되는 방향’으로 나아가야 하는데, 이 말은 가중치에서 기울기를 뺐을 때 가중치의 변화가 전혀 없는 상태를 말함, 
오차 역전파를 다른 방식으로 표현하면 가중치에서 기울기를 빼도 값의 변화가 없을 때까지 계속해서 가중치 수정 작업을 반복하는 것

<img src="https://user-images.githubusercontent.com/54765256/90968874-9509f180-e52c-11ea-8959-342ec92314d4.png">

## 신경망에서 딥러닝으로

다층 퍼셉트론이 오차 역전파를 만나 신경망이 되었고, 신경망은 XOR문제를 가볍게 해결함, 
이제 신경망을 차곡차곡 쌓아올리면 마치 사람처럼 생각하고 판단하는 인공지능이 금방이라도 완성될 것처럼 보임

## 기울기 소실 문제와 활성화 함수

<img src="https://user-images.githubusercontent.com/54765256/90968899-f762f200-e52c-11ea-84ba-b02765e3e592.png">

기울기 소실(vanishing gradient) 문제가 발생하기 시작한 것은 활성화 함수로 사용된 시그모이드 함수의 특성 때문임

여러 층을 거칠수록 기울기가 사라져 가중치를 수정하기가 어려워지는 것임

그림 9-3  시그모이드의 미분

<img src="https://user-images.githubusercontent.com/54765256/90968906-15305700-e52d-11ea-937f-4281ea33d2dd.png">

이를 해결하고자 활성화 함수를 시그모이드가 아닌 여러 함수로 대체하기 시작함

<img src="https://user-images.githubusercontent.com/54765256/90968930-50cb2100-e52d-11ea-8358-0dd00b323e31.png">

그림 9-4  여러 활성화 함수의 도입

### 하이퍼볼릭 탄젠트(tanh) 함수

미분한 값의 범위가 함께 확장되는 효과를 가져옴

다양한 여전히 1보다 작은 값이 존재하므로 기울기 소실 문제는 사라지지 않음

### 렐루(ReLU) 함수

시그모이드 함수의 대안으로 떠오르며 현재 가장 많이 사용되는 활성화 함수임

여러 은닉층을 거치며 곱해지더라도 맨 처음 층까지 사라지지 않고 남아있을 수 있음

이 간단한 방법이 여러 층을 쌓을 수 있게 했고, 이로써 딥러닝의 발전에 속도가 붙게 됨

### 소프트플러스 (softplus) 함수

이후 렐루의 0이 되는 순간을 완화

## 속도와 정확도 문제를 해결하는 고급 경사 하강법

경사 하강법은 정확하게 가중치를 찾아가지만, 한 번 업데이트할 때마다 전체 데이터를 미분해야 하므로 계산량이 매우 많다는 단점이 있음

이러한 점을 보완한 고급 경사 하강법이 등장하면서 딥러닝의 발전 속도는 더 빨라짐

### 확률적 경사 하강법

경사 하강법은 불필요하게 많은 계산량은 속도를 느리게 할 뿐 아니라, 최적 해를 찾기 전에 최적화 과정이 멈출 수도 있음
, 경사 하강법의 이러한 단점을 보완한 방법

전체 데이터를 사용하는 것이 아니라, 랜덤하게 추출한 일부 데이터를 사용함

일부 데이터를 사용하므로 더 빨리 그리고 자주 업데이트를 하는 것이 가능해짐

랜덤한 일부 데이터를 사용하는 만큼 확률적 경사 하강법은 중간 결과의 진폭이 크고 불안정해 보일 수도 있음

속도가 확연히 빠르면서도 최적 해에 근사한 값을 찾아낸다는 장점 덕분에 경사 하강법의 대안으로 사용되고 있음

그림 9-5  경사 하강법과 확률적 경사 하강법의 비교

<img src="https://user-images.githubusercontent.com/54765256/90968969-d6e76780-e52d-11ea-82ca-5e6cc958dd7a.png">

### 모멘텀

모멘텀(momentum)이란 단어는 ‘관성, 탄력, 가속도’라는 뜻

모멘텀 SGD란 말 그대로 경사 하강법에 탄력을 더해 주는 것

다시 말해서, 경사 하강법과 마찬가지로 매번 기울기를 구하지만, 이를 통해 오차를 수정하기 전 바로 앞 수정 값과 방향(+, -)을 참고하여 같은 방향으로 일정한 비율만 수정되게 하는 방법

수정 방향이 양수(+) 방향으로 한 번, 음수(-) 방향으로한 번 지그재그로 일어나는 현상이 줄어들고, 이전 이동 값을 고려하여 일정 비율만큼만  효과를 낼 수 있음

<img src="https://user-images.githubusercontent.com/54765256/90968979-fbdbda80-e52d-11ea-904b-bdc1679ddc66.png">

<img src="https://user-images.githubusercontent.com/54765256/90969001-29288880-e52e-11ea-94a8-b857a70e25ff.png">

<img src="https://user-images.githubusercontent.com/54765256/90969007-43626680-e52e-11ea-9202-eca7f974511d.png">





